# sstable-to-arrow

This folder contains the source code for sstable-to-arrow.

- `ksy/` contains the Kaitai Struct declarations for the various SSTable classes.
- `util/` contains different "opaque types" (types defined outside of kaitai) used by the Kaitai Struct classes, as well as classes to help parse and transform the data.

Be warned that error handling and logging are extremely rudimentary.

## How to run

This project can be run through a [Docker](https://www.docker.com/) container via
```bash
# replace /path/to/sstable/directory with the path to the directory with your sstables
docker run --rm -itp 9143:9143 -v /path/to/sstable/directory:/mnt/sstables --name sstable-to-arrow datastaxlabs/sstable-to-arrow /mnt/sstables
```

With the VS Code [Remote - Containers](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) extension installed, you can also open this folder in VS Code, run `Open Folder in Container` from the command palette, and select this folder to run the project within a Docker container.

If not using Docker, you can manually build the project as follows, though installation of dependencies may vary from machine to machine. You can see the [Dockerfile](Dockerfile) for example commands on a Linux machine.

1. This project depends on [Kaitai Struct](`https://kaitai.io/#download`), the [Kaitai Struct C++/STL runtime library](https://github.com/kaitai-io/kaitai_struct_cpp_stl_runtime), and [Apache Arrow for C++](http://arrow.apache.org/docs/cpp/cmake.html). If you are manually building Arrow and using other Arrow features like the filesystem interface `arrow::fs`, make sure to check [if you need to include any optional components](http://arrow.apache.org/docs/developers/cpp/building.html#optional-components). **This project requires `-DARROW_PARQUET=ON`.** See the `RUN` commands in the [`Dockerfile`](Dockerfile) for an example.

2. Get (an) SSTable(s). You can download IOT (Internet of Things) sample data generated by [NoSQLBench](http://docs.nosqlbench.io/#/) [at this Google Drive folder](https://drive.google.com/drive/folders/1y-f6rRH3OfC8AvVTNuhcmvjihnaMWN4p?usp=sharing). You can also generate IOT data using the script at [../test/generate-data](../test/generate-data), or you can manually create a table using CQL and the Cassandra Docker image using the steps below. See [the Cassandra quickstart](https://cassandra.apache.org/quickstart/) for more info.

```bash
docker network create cassandra
docker run --rm -d --name cassandra --hostname cassandra --network cassandra cassandra:3.11
# Run a CQL query to create the data. You may need to wait for the server to start up before running this.
# If you have a CQL query in a file, run:
# docker run --rm --network cassandra -v "<LOCAL_PATH_TO_CQL_FILE>:/scripts/data.cql" -e CQLSH_HOST=cassandra -e CQLSH_PORT=9042 nuvo/docker-cqlsh
# To open a CQL shell on the container, run:
# docker run --rm -it --network cassandra nuvo/docker-cqlsh cqlsh cassandra 9042 --cqlversion='3.4.4'
docker exec cassandra /opt/cassandra/bin/nodetool flush
docker cp cassandra:/var/lib/cassandra/data/<YOUR_KEYSPACE> ./res
# clean up
docker kill cassandra
docker network rm cassandra
```

3. Compile as follows:

```bash
mkdir build
cd build
cmake .. # or if using ninja: cmake -GNinja ..
make # or: ninja
```

4. Run:

```bash
./sstable_to_arrow <PATH_TO_SSTABLE_DIRECTORY>
```

This will listen for a connection on port 9143. It expects the client to send a
message first, and then it will send data in the following format:

1. The number of Arrow Tables being transferred, an 8-byte big-endian unsigned integer
2. For each table:
    1. Its size in bytes as an 8-byte big-endian unsigned integer
    2. The contents of the table in Arrow IPC Stream Format

## Limitations and caveats

- sstable-to-arrow does not do deduping and sends each SSTable as an Arrow Table. The user must configure a cuDF per sstable and use the GPU to merge the sstables based on last write wins semantics. sstable-to-arrow exposes internal cassandra timestamps so that merging can be done at the cuDF layer. Support for tombstones is currently under development.
- Some information, namely the names of the partition key and clustering columns, can't actually be deduced from the SSTable files, so they are represented in the table as simply `partition key` and `clustering key`.
- Cassandra stores data in memtables and commitlog before flushing to sstables, so analytics performed via only sstable-to-arrow will potentially be stale / not real-time.
- Currently, the parser has only been tested with SSTables written by Cassandra OSS 3.11. These *should* be identical to SSTables written by Cassandra 3.x.
- The system is set up to scan entire sstables (not read specific partitions). More work will be needed if we ever do predicate pushdown.
- The following cql types are not supported: `counter`, `frozen`, and user-defined types.
- `varint`s can only store up to 8 bytes. Attempting to read a table with larger `varint`s will crash.
- The parser can only read tables with up to 64 columns.
- `decimal`s are converted into an 8-byte floating point value because neither C++ nor Arrow has native support for arbitrary-precision integers or decimals like of the Java `BigInteger` or `BigDecimal` classes. This means that operations on decimal columns will use floating point arithmetic, which may be inexact.
- `set`s are treated as lists since Arrow has no equivalent of a set.

## TODO

- Fix or improve limitations above
    - Add support for tombstones
- Improve error handling system and logging system
- Add documentation
- Integrate with S3
- Include bindings that can be called from Python
